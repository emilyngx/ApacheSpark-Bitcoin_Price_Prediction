{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4550770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "## @type: DataSource\n",
    "## @args: [database = \"weather\", table_name = \"crawler_weather_small\", transformation_ctx = \"datasource0\"]\n",
    "## @return: datasource0\n",
    "## @inputs: []\n",
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"weather\", table_name = \"crawler_weather_small\", transformation_ctx = \"datasource0\")\n",
    "## @type: ApplyMapping\n",
    "## @args: [mapping = [(\"col0\", \"long\", \"col0\", \"long\"), (\"col1\", \"string\", \"col1\", \"string\"), (\"col2\", \"string\", \"col2\", \"string\"), (\"col3\", \"string\", \"col3\", \"string\"), (\"col4\", \"string\", \"col4\", \"string\"), (\"col5\", \"string\", \"col5\", \"string\"), (\"col6\", \"string\", \"col6\", \"string\"), (\"col7\", \"string\", \"col7\", \"string\"), (\"col8\", \"string\", \"col8\", \"string\"), (\"col9\", \"string\", \"col9\", \"string\"), (\"col10\", \"string\", \"col10\", \"string\"), (\"col11\", \"string\", \"col11\", \"string\"), (\"col12\", \"string\", \"col12\", \"string\"), (\"col13\", \"string\", \"col13\", \"string\"), (\"col14\", \"string\", \"col14\", \"string\")], transformation_ctx = \"applymapping1\"]\n",
    "## @return: applymapping1\n",
    "## @inputs: [frame = datasource0]\n",
    "applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [(\"col0\", \"long\", \"col0\", \"long\"), (\"col1\", \"string\", \"col1\", \"string\"), (\"col2\", \"string\", \"col2\", \"string\"), (\"col3\", \"string\", \"col3\", \"string\"), (\"col4\", \"string\", \"col4\", \"string\"), (\"col5\", \"string\", \"col5\", \"string\"), (\"col6\", \"string\", \"col6\", \"string\"), (\"col7\", \"string\", \"col7\", \"string\"), (\"col8\", \"string\", \"col8\", \"string\"), (\"col9\", \"string\", \"col9\", \"string\"), (\"col10\", \"string\", \"col10\", \"string\"), (\"col11\", \"string\", \"col11\", \"string\"), (\"col12\", \"string\", \"col12\", \"string\"), (\"col13\", \"string\", \"col13\", \"string\"), (\"col14\", \"string\", \"col14\", \"string\")], transformation_ctx = \"applymapping1\")\n",
    "\n",
    "#------------------------------\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#Convert DynamicFrame to Sprk DataFrame\n",
    "df = applymapping1.toDF()\n",
    "\n",
    "new_cols = ['id', 'Station', 'WBAN', 'Year', 'Month', 'Day', 'Hour', 'Temperature', 'Dew_Point', 'Pressure', 'Wind_Direction', 'Wind_Speed', 'Sky_Condition', 'Precipitation_1H', 'Precipitation_6H']\n",
    "for i, col in enumerate(df.columns):\n",
    "    df = df.withColumnRenamed(col, new_cols[i])\n",
    "\n",
    "#Drop the row containing column names\n",
    "print('size before filter:', df.count())\n",
    "header_removed_DF = df.filter(F.col(new_cols[1]) != new_cols[1])\n",
    "print('size after filter:', header_removed_DF.count())\n",
    "\n",
    "#replace missing values with None\n",
    "for col in ['Temperature', 'Dew_Point','Wind_Direction', 'Wind_Speed']:\n",
    "    header_removed_DF = header_removed_DF.withColumn(col,F.when(F.col(col).isin([9999, -9999, 999, -999]),None).otherwise(F.col(col)))\n",
    "#for pressure, only [-9999, -999] indicate missing values\n",
    "header_removed_DF = header_removed_DF.withColumn('Pressure',F.when(F.col('Pressure').isin([-9999, -999]),None).otherwise(F.col('Pressure')))\n",
    "\n",
    "#Divide some measures by 10\n",
    "for col in ['Temperature', 'Dew_Point','Wind_Direction', 'Wind_Speed', 'Pressure']:\n",
    "    header_removed_DF = header_removed_DF.withColumn(col, F.col(col)/10.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a619665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column named station_WBAN that combines Station and WBAN columns and separates the values using a hyphen\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "header_removed_DF = header_removed_DF.withColumn('station_WBAN', F.concat(F.col('Station'),F.lit('-'),F.col('WBAN')))\n",
    "\n",
    "#For precipitation columns, replace all missing values with zero.\n",
    "for col in ['Precipitation_1H', 'Precipitation_6H']:\n",
    "     header_removed_DF = header_removed_DF.withColumn(col,F.when(F.col(col).isin([9999, -9999, 999, -999]),0).otherwise(F.col(col)))\n",
    "\n",
    "#For all other columns, replace the missing values with the mean of the measure for the corresponding station\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, when, col\n",
    "w = Window.partitionBy(header_removed_DF.Station)\n",
    "for col in ['Temperature', 'Dew_Point', 'Pressure', 'Wind_Direction', 'Wind_Speed', 'Sky_Condition']:\n",
    "    header_removed_DF = header_removed_DF.withColumn(col,avg(F.col(col)).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda66186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#End Assignment 3 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba40766",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_removed_DF.show()\n",
    "print(header_removed_DF.select('Year').distinct().collect())\n",
    "\n",
    "#convert back to DynamicFrame\n",
    "newDynamicDF = DynamicFrame.fromDF(header_removed_DF, glueContext, 'clean_df')\n",
    "\n",
    "#-------------------------------\n",
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", connection_options = {\"path\": \"s3://gba6430emily/Glue/weather_transformed\", \"compression\": \"gzip\"}, format = \"csv\", transformation_ctx = \"datasink2\"]\n",
    "## @return: datasink2\n",
    "## @inputs: [frame = applymapping1]\n",
    "datasink2 = glueContext.write_dynamic_frame.from_options(frame = newDynamicDF, connection_type = \"s3\", connection_options = {\"path\": \"s3://gba6430emily/Glue/weather_transformed\", \"compression\": \"gzip\"}, format = \"csv\", transformation_ctx = \"datasink2\")\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
